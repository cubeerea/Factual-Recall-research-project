{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers - Advanced Topics\n",
    "\n",
    "This notebook extends your basic transformer with:\n",
    "1. Tokenization (BPE)\n",
    "2. Sinusoidal positional encodings\n",
    "3. Training loop\n",
    "4. Text generation strategies\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Tokenization - How Text Becomes Numbers\n",
    "\n",
    "Before transformers can process text, we need to convert it to integers.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    Simple word-level tokenizer.\n",
    "    \n",
    "    In practice, you'd use:\n",
    "    - BPE (Byte Pair Encoding) - GPT\n",
    "    - WordPiece - BERT  \n",
    "    - SentencePiece - T5, LLaMA\n",
    "    \n",
    "    But let's start simple to understand the concept.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def build_vocab(self, texts, max_vocab=1000):\n",
    "        \"\"\"Build vocabulary from list of texts\"\"\"\n",
    "        # Tokenize: split on whitespace and punctuation\n",
    "        all_words = []\n",
    "        for text in texts:\n",
    "            words = re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
    "            all_words.extend(words)\n",
    "        \n",
    "        # Count frequencies\n",
    "        word_counts = Counter(all_words)\n",
    "        \n",
    "        # Take most common words\n",
    "        most_common = word_counts.most_common(max_vocab - 4)\n",
    "        \n",
    "        # Special tokens\n",
    "        self.word_to_id = {\n",
    "            '<PAD>': 0,   # padding token\n",
    "            '<UNK>': 1,   # unknown token\n",
    "            '<BOS>': 2,   # beginning of sequence\n",
    "            '<EOS>': 3    # end of sequence\n",
    "        }\n",
    "        \n",
    "        # Add vocabulary\n",
    "        for i, (word, _) in enumerate(most_common):\n",
    "            self.word_to_id[word] = i + 4\n",
    "        \n",
    "        # Reverse mapping\n",
    "        self.id_to_word = {v: k for k, v in self.word_to_id.items()}\n",
    "        self.vocab_size = len(self.word_to_id)\n",
    "        \n",
    "        print(f\"Built vocabulary of {self.vocab_size} tokens\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to list of token IDs\"\"\"\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
    "        ids = [self.word_to_id.get(w, self.word_to_id['<UNK>']) for w in words]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert list of token IDs back to text\"\"\"\n",
    "        words = [self.id_to_word.get(i, '<UNK>') for i in ids]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Demo\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The dog was lazy but the fox was quick.\",\n",
    "    \"A brown fox and a brown dog.\"\n",
    "]\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(texts, max_vocab=50)\n",
    "\n",
    "sample_text = \"The fox jumps\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nOriginal: {sample_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding (BPE) - Used in GPT\n",
    "\n",
    "BPE learns subword units. Why?\n",
    "- Unknown words: \"unhappiness\" â†’ \"un\" + \"happiness\"\n",
    "- Rare words: Split into known pieces\n",
    "- Efficient vocabulary: Balance between character and word level\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPE:\n",
    "    \"\"\"\n",
    "    Simplified Byte Pair Encoding.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Start with character vocabulary\n",
    "    2. Find most frequent pair of adjacent tokens\n",
    "    3. Merge that pair into a new token\n",
    "    4. Repeat until desired vocab size\n",
    "    \"\"\"\n",
    "    def __init__(self, num_merges=100):\n",
    "        self.num_merges = num_merges\n",
    "        self.merges = {}  # (a, b) -> ab\n",
    "        self.vocab = set()\n",
    "    \n",
    "    def get_pairs(self, word):\n",
    "        \"\"\"Get all adjacent pairs in a word\"\"\"\n",
    "        pairs = set()\n",
    "        prev = word[0]\n",
    "        for char in word[1:]:\n",
    "            pairs.add((prev, char))\n",
    "            prev = char\n",
    "        return pairs\n",
    "    \n",
    "    def train(self, texts):\n",
    "        \"\"\"Learn BPE merges from texts\"\"\"\n",
    "        # Split into words and characters\n",
    "        vocab = {}\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                word_chars = tuple(word) + ('</w>',)  # end of word marker\n",
    "                vocab[word_chars] = vocab.get(word_chars, 0) + 1\n",
    "        \n",
    "        # Perform merges\n",
    "        for i in range(self.num_merges):\n",
    "            # Count all pairs\n",
    "            pair_counts = {}\n",
    "            for word, freq in vocab.items():\n",
    "                pairs = self.get_pairs(word)\n",
    "                for pair in pairs:\n",
    "                    pair_counts[pair] = pair_counts.get(pair, 0) + freq\n",
    "            \n",
    "            if not pair_counts:\n",
    "                break\n",
    "            \n",
    "            # Find most frequent pair\n",
    "            best_pair = max(pair_counts, key=pair_counts.get)\n",
    "            \n",
    "            # Merge this pair in vocabulary\n",
    "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "            \n",
    "            # Update vocabulary\n",
    "            new_vocab = {}\n",
    "            for word in vocab:\n",
    "                new_word = self.merge_pair(word, best_pair)\n",
    "                new_vocab[new_word] = vocab[word]\n",
    "            vocab = new_vocab\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                print(f\"Merge {i}: {best_pair} -> {best_pair[0] + best_pair[1]}\")\n",
    "        \n",
    "        # Extract final vocabulary\n",
    "        for word in vocab:\n",
    "            self.vocab.update(word)\n",
    "        \n",
    "        print(f\"\\nFinal vocab size: {len(self.vocab)}\")\n",
    "    \n",
    "    def merge_pair(self, word, pair):\n",
    "        \"\"\"Merge a specific pair in a word\"\"\"\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and (word[i], word[i+1]) == pair:\n",
    "                new_word.append(pair[0] + pair[1])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        return tuple(new_word)\n",
    "    \n",
    "    def encode_word(self, word):\n",
    "        \"\"\"Encode a word using learned merges\"\"\"\n",
    "        word = tuple(word) + ('</w>',)\n",
    "        \n",
    "        # Apply all merges in order\n",
    "        for pair in self.merges:\n",
    "            word = self.merge_pair(word, pair)\n",
    "        \n",
    "        return list(word)\n",
    "\n",
    "# Demo BPE\n",
    "bpe = SimpleBPE(num_merges=50)\n",
    "bpe.train(texts)\n",
    "\n",
    "print(\"\\nEncoding examples:\")\n",
    "for word in [\"quick\", \"lazy\", \"jumping\"]:\n",
    "    encoded = bpe.encode_word(word)\n",
    "    print(f\"{word:10s} -> {encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Positional Encodings - Sinusoidal vs Learned\n",
    "\n",
    "Transformers have no inherent notion of order. We must inject position information.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_position_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    Original transformer paper used sinusoidal positional encoding.\n",
    "    \n",
    "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \n",
    "    Why sine/cosine?\n",
    "    - Model can learn to attend to relative positions\n",
    "    - Generalizes to longer sequences than seen in training\n",
    "    - No learned parameters\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "    \n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                        -(math.log(10000.0) / d_model))\n",
    "    \n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# Visualize sinusoidal positional encoding\n",
    "max_len = 100\n",
    "d_model = 128\n",
    "\n",
    "pe = sinusoidal_position_encoding(max_len, d_model)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap\n",
    "im = axes[0].imshow(pe.numpy(), aspect='auto', cmap='RdBu')\n",
    "axes[0].set_xlabel('Embedding Dimension')\n",
    "axes[0].set_ylabel('Position')\n",
    "axes[0].set_title('Sinusoidal Positional Encoding')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Individual positions\n",
    "for pos in [0, 10, 20, 50]:\n",
    "    axes[1].plot(pe[pos, :64].numpy(), label=f'Position {pos}', alpha=0.7)\n",
    "axes[1].set_xlabel('Dimension')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('Encoding for Different Positions')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Each position has a unique pattern\")\n",
    "print(\"Low dimensions = slow oscillation, High dimensions = fast oscillation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sinusoidal vs learned embeddings\n",
    "# Learned (what we used in your transformer)\n",
    "learned_pe = nn.Embedding(max_len, d_model)\n",
    "\n",
    "print(f\"Sinusoidal PE shape: {pe.shape}\")\n",
    "print(f\"Sinusoidal: 0 parameters, generalizes to unseen lengths\\n\")\n",
    "\n",
    "print(f\"Learned PE params: {sum(p.numel() for p in learned_pe.parameters()):,}\")\n",
    "print(f\"Learned: {max_len * d_model:,} parameters, fixed max length\")\n",
    "\n",
    "print(\"\\n| Type | Pros | Cons |\")\n",
    "print(\"|------|------|------|\")\n",
    "print(\"| Sinusoidal | No params, extrapolates | Fixed formula |\")\n",
    "print(\"| Learned | Flexible, data-driven | Doesn't extrapolate, needs training |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Training Loop - Actually Train the Transformer\n",
    "\n",
    "Let's train on a simple task: learning to copy sequences.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your transformer from the main notebook\n",
    "# For this demo, we'll use a simplified version\n",
    "\n",
    "from transformers import TwoLayerTransformer  # assumes you saved it\n",
    "\n",
    "# Or copy-paste the class here if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create toy dataset: learn to repeat sequences\n",
    "def create_copy_dataset(n_samples=1000, seq_len=10, vocab_size=20):\n",
    "    \"\"\"\n",
    "    Simple task: given [1, 5, 3, ...], predict [1, 5, 3, ...]\n",
    "    \n",
    "    This tests if the transformer can learn to copy.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for _ in range(n_samples):\n",
    "        seq = torch.randint(1, vocab_size, (seq_len,))\n",
    "        data.append(seq)\n",
    "    return torch.stack(data)\n",
    "\n",
    "# Generate data\n",
    "vocab_size = 50\n",
    "seq_len = 16\n",
    "train_data = create_copy_dataset(n_samples=500, seq_len=seq_len, vocab_size=vocab_size)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Sample sequence: {train_data[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = TwoLayerTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    d_ff=512,\n",
    "    max_seq_len=seq_len,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "losses = []\n",
    "batch_size = 32\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, len(train_data), batch_size):\n",
    "        batch = train_data[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(batch)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        # Loss: predict next token at each position\n",
    "        # Teacher forcing: use ground truth for input\n",
    "        loss = criterion(\n",
    "            logits[:, :-1].reshape(-1, vocab_size),  # predictions for positions 0 to n-1\n",
    "            batch[:, 1:].reshape(-1)                  # targets: tokens at positions 1 to n\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (prevents exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / n_batches\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Transformer Training on Copy Task')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "model.eval()\n",
    "test_seq = train_data[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(test_seq.unsqueeze(0))\n",
    "    predictions = torch.argmax(logits, dim=-1).squeeze()\n",
    "\n",
    "print(\"\\nTest on training example:\")\n",
    "print(f\"Input:      {test_seq.tolist()}\")\n",
    "print(f\"Predicted:  {predictions.tolist()}\")\n",
    "print(f\"Target:     {test_seq.tolist()[1:] + [0]}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = (predictions[:-1] == test_seq[1:]).sum().item()\n",
    "total = len(test_seq) - 1\n",
    "print(f\"\\nAccuracy: {correct}/{total} = {100*correct/total:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Generation Strategies\n",
    "\n",
    "How to generate text from a trained transformer.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedy(model, start_tokens, max_len=20, pad_token=0):\n",
    "    \"\"\"\n",
    "    Greedy decoding: always pick the most likely next token.\n",
    "    \n",
    "    Simple but leads to repetitive text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = start_tokens.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - len(tokens)):\n",
    "            # Get logits for all positions\n",
    "            logits = model(tokens.unsqueeze(0))  # (1, seq_len, vocab)\n",
    "            \n",
    "            # Take logits for last position\n",
    "            next_token_logits = logits[0, -1, :]  # (vocab,)\n",
    "            \n",
    "            # Pick most likely token\n",
    "            next_token = torch.argmax(next_token_logits)\n",
    "            \n",
    "            # Append to sequence\n",
    "            tokens = torch.cat([tokens, next_token.unsqueeze(0)])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temperature(model, start_tokens, max_len=20, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Temperature sampling:\n",
    "    - temperature > 1: more random (more diversity)\n",
    "    - temperature < 1: more confident (less diversity)\n",
    "    - temperature = 1: sample from true distribution\n",
    "    \n",
    "    Formula: p_i = exp(logit_i / T) / sum(exp(logit_j / T))\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = start_tokens.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - len(tokens)):\n",
    "            logits = model(tokens.unsqueeze(0))\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Sample from probability distribution\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            tokens = torch.cat([tokens, next_token])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_top_k(model, start_tokens, max_len=20, k=10):\n",
    "    \"\"\"\n",
    "    Top-k sampling: only sample from k most likely tokens.\n",
    "    \n",
    "    Prevents sampling very unlikely tokens (reduces nonsense).\n",
    "    Used in GPT-2.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = start_tokens.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - len(tokens)):\n",
    "            logits = model(tokens.unsqueeze(0))\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Keep only top k logits\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, k)\n",
    "            \n",
    "            # Sample from top k\n",
    "            probs = F.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            tokens = torch.cat([tokens, next_token])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_top_p(model, start_tokens, max_len=20, p=0.9):\n",
    "    \"\"\"\n",
    "    Top-p (nucleus) sampling: sample from smallest set of tokens\n",
    "    whose cumulative probability >= p.\n",
    "    \n",
    "    Used in GPT-3, ChatGPT.\n",
    "    Adaptive: fewer tokens when model is confident, more when uncertain.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = start_tokens.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - len(tokens)):\n",
    "            logits = model(tokens.unsqueeze(0))\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Sort by probability\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "            \n",
    "            # Find cumulative probability\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "            \n",
    "            # Remove tokens below threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > p\n",
    "            # Keep at least one token\n",
    "            if sorted_indices_to_remove[0]:\n",
    "                sorted_indices_to_remove[0] = False\n",
    "            else:\n",
    "                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "                sorted_indices_to_remove[0] = False\n",
    "            \n",
    "            # Zero out removed indices\n",
    "            sorted_probs[sorted_indices_to_remove] = 0\n",
    "            sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "            \n",
    "            # Sample\n",
    "            next_token_idx = torch.multinomial(sorted_probs, num_samples=1)\n",
    "            next_token = sorted_indices[next_token_idx]\n",
    "            \n",
    "            tokens = torch.cat([tokens, next_token])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo all sampling methods\n",
    "start = torch.tensor([1, 5, 8, 3])\n",
    "\n",
    "print(\"Starting tokens:\", start.tolist())\n",
    "print(\"\\nGeneration methods:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(f\"Greedy:           {generate_greedy(model, start, max_len=12).tolist()}\")\n",
    "print(f\"Temp=0.5 (safe):  {generate_temperature(model, start, max_len=12, temperature=0.5).tolist()}\")\n",
    "print(f\"Temp=1.0 (std):   {generate_temperature(model, start, max_len=12, temperature=1.0).tolist()}\")\n",
    "print(f\"Temp=1.5 (wild):  {generate_temperature(model, start, max_len=12, temperature=1.5).tolist()}\")\n",
    "print(f\"Top-k (k=5):      {generate_top_k(model, start, max_len=12, k=5).tolist()}\")\n",
    "print(f\"Top-p (p=0.9):    {generate_top_p(model, start, max_len=12, p=0.9).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Strategy Comparison\n",
    "\n",
    "| Method | Description | Pros | Cons | Used In |\n",
    "|--------|-------------|------|------|--------|\n",
    "| Greedy | Pick argmax | Fast, deterministic | Repetitive, no diversity | Debugging |\n",
    "| Temperature | Scale logits | Controls randomness | Samples bad tokens | Tuning creativity |\n",
    "| Top-k | Sample from k best | Filters unlikely tokens | Fixed cutoff | GPT-2 |\n",
    "| Top-p | Sample from cumulative p | Adaptive cutoff | More complex | GPT-3, ChatGPT |\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You now understand:\n",
    "1. **Tokenization**: Word-level and BPE (subword)\n",
    "2. **Positional encodings**: Sinusoidal (generalizes) vs Learned (flexible)\n",
    "3. **Training**: Cross-entropy loss, teacher forcing, gradient clipping\n",
    "4. **Generation**: Greedy, temperature, top-k, top-p sampling\n",
    "\n",
    "**Next steps for building your own library:**\n",
    "- Implement encoder-decoder architecture (for translation)\n",
    "- Add Pre-LN vs Post-LN (layer norm placement)\n",
    "- Implement KV caching (for efficient generation)\n",
    "- Understand Flash Attention (memory-efficient attention)\n",
    "- Study ROPE (Rotary Position Embeddings) - used in LLaMA\n",
    "\n",
    "**Your notes here:**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
