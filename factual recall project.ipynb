{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac59428-51f9-477f-9572-7f29eae764c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccf3ac-4859-421c-a654-7fc5aad96993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" \n",
    "        \n",
    "        self.d_model = d_model # The dimensionality of all representations\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k)\"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine heads back to original dimension\"\"\"\n",
    "        batch_size, num_heads, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "    ## Forward pass\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (batch, num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask (for causal attention) \n",
    "        # This is to zero out all future positions, forcing the predictions to use past and present information only, which is especially important for text generation.   \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Combine heads\n",
    "        attn_output = self.combine_heads(attn_output)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e33dc4-791f-43ad-a42c-4167fed42e9a",
   "metadata": {},
   "source": [
    "`d_model` --> the dimsensionality of all representations\n",
    "- if `d_model` = 256\n",
    "  - each token embedding is a 256-dimensional vector\n",
    "  - each position embedding is 256-dimensional\n",
    "  - output of each attention layer is 256-dimensional\n",
    "  - everything flow through the model is 256-dimensional space\n",
    "  - Memory size of the model\n",
    "- Think 'width' of a neural network - how much information capacity each token holds.\n",
    "  - Each token's size depends on d_model's vector size\n",
    "- So is larger better?\n",
    "  - Allows each token to store more information, more working memory, and allows more heads.\n",
    "  - BUT, the weights scale with d_model so this means more parameters, more computation, and more memory\n",
    "-Scales in quadratically in parameters -> d_model*d_model or d_model^2\n",
    "\n",
    "`num_heads / d_k` --> number of attention heads\n",
    "- Used to split attention operations on the d_model \n",
    "- the dimension of key (as well as query and value) vectors \n",
    "  - (`d_k` = `d_model` / `num_heads`)\n",
    "- d_model = 256, heads = 8 -> 32 dimension heads to work with\n",
    "  -Head 1: 0-31, Head 2: 32-63, etc...\n",
    "  -Different heads learn different patterns, head 1 might learn subject-verb, head 2 might learn adjective-noun\n",
    "-Concatenate all heads back together later to get the full d_model vector\n",
    "\n",
    "`nn.Linear` --> Projects into matrix multiplication\n",
    "- Creates a weight matrix\n",
    "  - Let `d_model = 4` then W_q weight matrix is now 4x4\n",
    "- Gradient descent is related as it updates the weights inside W_q, W_k and W_v to make better predictions (Backward Pass). But projection itself the `nn.Linear` is matrix multiplication (Forward Pass)\n",
    "\n",
    "`Q`, `K`, `V` , `O`\n",
    "- `Q` -> Asks a question on what information is needed. \"What am I looking for?\"\n",
    "- `K` -> Key mapping to the question. \"What do I contain?\"\n",
    "- `V` -> The content of the key. Think key-value pair in python `dict`. \"What information do I have?\"\n",
    "- Think soft database lookup\n",
    "- `O` -> Takes concatenated attention outputs from all heads and mixes them\n",
    "- Full Flow is as follows \n",
    "  - Input (d_model=512) -> [W_q, W_k, W_v] is split into 8 heads (64 dim) -> Attention computation per head -> Concatenate back to 512 dim -> W_o final output at 512 dim\n",
    "\n",
    "`seq_len` --> Token Length\n",
    "- Attention computation is at O(n^2) complexity, scales in attention\n",
    "- While `d_model` holds information capacity for token (How smart is each token), `seq_len` is how much context it can see (How much can it remember at once)\n",
    "- **Note** That a token is roughly 0.75 of word. \n",
    "\n",
    "`Forward Function Walkthrough` \n",
    "- 1. Set up -> batch=2, seq_len=4, d_mode=512, num_heads=8, d_k=64 \n",
    "- 2. Linear Projections -> x=(2,4,512) - `batch, seq_len, dim` , project into Q, K, V Spaces\n",
    "- 3. Split into heads -> Before x=(2,4,512), After x=(2,8,4,64) - `batch, num_heads, seq_len, d_k` to now have 8 independent attention mechanisms with 64 dimensions each\n",
    "- **Important - Core Mechanism** Scaled dot-product attention \n",
    "  1. Compute attention Scores -> Measure how much each token should focus on every other token\n",
    "    - `K.transpose(-2,-1):` Swap last to dims -> K = (batch, num_heads, d_k, seq_len)\n",
    "    - `Q @ K.T:` Becomes (batch, num_heads, seq_len, seq_len). If Q and K point in similar directions, their dot product is high = strong attention. Shape also follows (seq_len, seq_len) so for example = 3, then matrix is (3,3)\n",
    "  2. Scale the scores -> As d_k gets larger, dot product gets larger, more dims to sum\n",
    "    - Large scores -> softmax saturates -> gardients vanish\n",
    "    - `sqrt(d_k):` To normalize, as if `Q` and `K` have unit variance, their dot product has variance = d_k. Therefore dividing by the square root normalizes the variance back to 1. \n",
    "    - Variance grows linearly with d_k as Var(Q*k) = Var(Q[0] * k[0]) + Var...\n",
    "      - Independent variables with variance = 1 + 1 + 1... = d_k\n",
    "  3. Apply mask if provided -> A boolean tensor indicating which positions to ignore\n",
    "    - `-1e9/-inf instead of 0:` Softmax scores are still applied to padding if set to 0, but setting at -inf allows padding to be ignored.  \n",
    "    ` Example: (mask = [1, 1, 0, 0]), scores = [2.5, 1.3, -1e9, -1e9]. Last 2 positions masked\n",
    "  4. Softmax to get attention weight -> Converts scores into pdf (sums to 1)\n",
    "    - **Refer to quick lesson on softmax**\n",
    "    - Allows gradients to flow smoothly\n",
    "      - Too small = No learning, Too large = unstable.\n",
    "      - Gradients of softmax (derivative of softmax) bound between **0 and 0.25**\n",
    "      - Average gradient often around 0.1-0.15\n",
    "    - Amplifies differences as larger scores get proportionally more weight\n",
    "      - Before `[2.0, 4.0, 1.0]` 4.0 is 2x bigger than 2.0, After `[0.12, 0.84, 0.04] ,84 is 7x bigger than 0.12\n",
    "      - `dim=-1:` Normalize on the rows for each key, based off indexing.\n",
    "        - Shape: (Batch=2, num_heads=8, seq_len=4, seq_len=4)\n",
    "        - dim=0: batch, dim=1: num_heads, dim=2: seq_len (queries), dim=3: seq_len (keys)\n",
    "        - dim=-4: batch, dim=-3:num_heads, dim=-2: seq_len (queries), **dim=-1: seq_len (keys)** \n",
    "        - This means for each query, we normalize across all keys\n",
    "  5. Apply attention to values -> compute weighted average of value vectors and mix\n",
    "    - Attention weights from softmax * value vectors\n",
    "    - `Example:` seq_len ='The', 'Cat', 'Sat', d_k = 4 -> value vectors for V_The, V_cat, V_sat -> weights from softmax for cat = [0.3, 0.38, 0.32], Compute weighted sum for a word -> 0.3 * V_The + 0.38 * V_cat + 0.32 * V_sat -> Cat is now is a mix of all tokens weighted by attention **Influenced 30% by \"The\", 38% by \"Cat/Itself\", 32% by \"Sat\"**\n",
    "- 4. Combine heads -> Before (2,8,4,64), After (2,4,512), concatenated back\n",
    "  - Uses .contiguous() to reorganize memory after transpose before reshaping\n",
    "- 5. Output projection -> Mix information across all heads into final output\n",
    "\n",
    "`Mask` --> Prevents attention to certain positions\n",
    "- Ignore padding tokens or can't look at future tokens\n",
    "\n",
    "Attention formula \n",
    "\n",
    "`scores = Q @ K^T / sqrt(d_k)`   How relevant is each token?\n",
    "\n",
    "`attention_weights = softmax(scores)`   Normalize to probabilities\n",
    "\n",
    "`output = attention_weights @ V`   Weighted sum of values\n",
    "\n",
    "Quick lesson on softmax \n",
    "- Given [0.9, -1.5, 3.2], we first want to exponentiate all values to make all values positive and amplify the differences -> [2.45960311116, 0.22313016014, 24.5325301971].\n",
    "- We then normalize by dividing based on the summation of the exponentials to find the probability distribution -> [2.45960311116/27.2152634684, 0.22313016014/27.2152634684, 24.5325301971/27.2152634684] --> [0.090376136, 0.008198731, 0.901425133] = 1\n",
    "\n",
    "`dropout`\n",
    "- Controls regularization.\n",
    "- In training, dropout randomly sets the percentage of neurons to zero.\n",
    "- If `dropout` = 0.1, 10% of neurons are deactivated and 90% remains activated.\n",
    "  - remaining neurons are scaled up and 'strengthened' to compensate\n",
    "- This prevents overfitting so that the training does not rely on a single/few neurons, making the model more robust.\n",
    "- Only activated during training, auto-disabled during inference/test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c680ca1-cfdf-4da4-9b10-78d4255752c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ec30c-a5f5-4b96-a0df-4fbd8ecd8095",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326d463-c325-497b-83f3-91fae84d19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afbb8c9-ed5d-453b-bd4e-9137b218e9f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007c2cf-528f-4a0f-97e8-de3aee5fc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, num_heads=4, d_ff=1024, \n",
    "                 max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        self.positional_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Two transformer layers\n",
    "        self.layer1 = TransformerLayer(d_model, num_heads, d_ff, dropout)\n",
    "        self.layer2 = TransformerLayer(d_model, num_heads, d_ff, dropout)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def create_causal_mask(self, seq_len, device):\n",
    "        \"\"\"Create a causal mask to prevent attending to future tokens\"\"\"\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "        return mask.view(1, 1, seq_len, seq_len)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # Create position indices\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
    "        \n",
    "        # Token + positional embeddings\n",
    "        token_emb = self.token_embedding(x)\n",
    "        pos_emb = self.positional_embedding(positions)\n",
    "        x = self.dropout(token_emb + pos_emb)\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = self.create_causal_mask(seq_len, x.device)\n",
    "        \n",
    "        # Pass through two transformer layers\n",
    "        x = self.layer1(x, mask)\n",
    "        x = self.layer2(x, mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a8d41-b581-46b6-b3d7-9ba69f4c04a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192a08d-dbf5-4b29-87ee-74dc2a623dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
