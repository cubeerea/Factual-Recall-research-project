{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks from Scratch\n",
    "\n",
    "Building foundational understanding of neural networks by implementing everything manually.\n",
    "\n",
    "**Goal**: Understand every computation that happens during forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Single Neuron (Perceptron / Logistic Regression)\n",
    "\n",
    "The simplest neural network: one neuron that learns to classify.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple 2D classification data\n",
    "# Two clusters of points we want to separate\n",
    "\n",
    "def generate_data(n_samples=100):\n",
    "    # Class 0: centered at (-1, -1)\n",
    "    X0 = np.random.randn(n_samples // 2, 2) * 0.5 + np.array([-1, -1])\n",
    "    # Class 1: centered at (1, 1)\n",
    "    X1 = np.random.randn(n_samples // 2, 2) * 0.5 + np.array([1, 1])\n",
    "    \n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = generate_data(200)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Class 0')\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Class 1')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.title('Binary Classification Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation\n",
    "\n",
    "Squashes any real number to range (0, 1) - interpretable as probability.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation: maps R -> (0, 1)\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid: sigmoid(z) * (1 - sigmoid(z))\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Visualize sigmoid and its derivative\n",
    "z = np.linspace(-6, 6, 100)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(z, sigmoid(z))\n",
    "axes[0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0].set_title('Sigmoid Function')\n",
    "axes[0].set_xlabel('z')\n",
    "axes[0].set_ylabel('sigmoid(z)')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(z, sigmoid_derivative(z))\n",
    "axes[1].set_title('Sigmoid Derivative (for backprop)')\n",
    "axes[1].set_xlabel('z')\n",
    "axes[1].set_ylabel(\"sigmoid'(z)\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Neuron: Forward Pass\n",
    "\n",
    "```\n",
    "z = w1*x1 + w2*x2 + b    (linear combination)\n",
    "a = sigmoid(z)            (activation)\n",
    "```\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNeuron:\n",
    "    def __init__(self, n_features):\n",
    "        # Initialize weights randomly, bias to zero\n",
    "        self.w = np.random.randn(n_features) * 0.01\n",
    "        self.b = 0.0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "        X: (n_samples, n_features)\n",
    "        Returns: predictions (n_samples,)\n",
    "        \"\"\"\n",
    "        # Linear combination: z = X @ w + b\n",
    "        self.z = np.dot(X, self.w) + self.b\n",
    "        # Activation\n",
    "        self.a = sigmoid(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Binary Cross-Entropy Loss:\n",
    "        L = -[y*log(p) + (1-y)*log(1-p)]\n",
    "        \n",
    "        Why this loss? It's derived from maximum likelihood estimation.\n",
    "        \"\"\"\n",
    "        # Clip to avoid log(0)\n",
    "        eps = 1e-15\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "        \n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Backward pass - compute gradients manually.\n",
    "        \n",
    "        Chain rule:\n",
    "        dL/dw = dL/da * da/dz * dz/dw\n",
    "        \n",
    "        For BCE + sigmoid, this simplifies beautifully:\n",
    "        dL/dz = a - y  (prediction error!)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Gradient of loss w.r.t. pre-activation (this is the key insight)\n",
    "        dz = self.a - y_true  # (n_samples,)\n",
    "        \n",
    "        # Gradient of loss w.r.t. weights\n",
    "        # dL/dw = X^T @ dz / n_samples\n",
    "        self.dw = np.dot(X.T, dz) / n_samples\n",
    "        \n",
    "        # Gradient of loss w.r.t. bias\n",
    "        self.db = np.mean(dz)\n",
    "        \n",
    "        return self.dw, self.db\n",
    "    \n",
    "    def update(self, lr=0.1):\n",
    "        \"\"\"Gradient descent update\"\"\"\n",
    "        self.w -= lr * self.dw\n",
    "        self.b -= lr * self.db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Binary prediction (threshold at 0.5)\"\"\"\n",
    "        return (self.forward(X) >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Single Neuron\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the single neuron\n",
    "neuron = SingleNeuron(n_features=2)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward\n",
    "    y_pred = neuron.forward(X)\n",
    "    loss = neuron.compute_loss(y, y_pred)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward\n",
    "    neuron.backward(X, y)\n",
    "    \n",
    "    # Update\n",
    "    neuron.update(lr=1.0)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        acc = np.mean(neuron.predict(X) == y)\n",
    "        print(f\"Epoch {epoch}: Loss={loss:.4f}, Accuracy={acc:.2%}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Class 0')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Class 1')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.legend()\n",
    "    plt.title('Decision Boundary (Single Neuron)')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(neuron, X, y)\n",
    "print(f\"Learned weights: w={neuron.w}, b={neuron.b:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Two-Layer MLP with Manual Backprop\n",
    "\n",
    "Now we add a hidden layer. This lets us learn non-linear decision boundaries.\n",
    "\n",
    "```\n",
    "Input (2) -> Hidden (4) -> Output (1)\n",
    "```\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XOR-like data (not linearly separable!)\n",
    "def generate_xor_data(n_samples=200):\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    # XOR: label is 1 if signs of x1 and x2 are different\n",
    "    y = ((X[:, 0] * X[:, 1]) < 0).astype(int)\n",
    "    return X, y\n",
    "\n",
    "X_xor, y_xor = generate_xor_data(300)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1], c='blue', label='Class 0')\n",
    "plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], c='red', label='Class 1')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.legend()\n",
    "plt.title('XOR Data (NOT linearly separable)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Math of Backpropagation\n",
    "\n",
    "For a 2-layer network:\n",
    "\n",
    "**Forward:**\n",
    "```\n",
    "z1 = X @ W1 + b1\n",
    "a1 = relu(z1)           # hidden activation\n",
    "z2 = a1 @ W2 + b2\n",
    "a2 = sigmoid(z2)        # output\n",
    "```\n",
    "\n",
    "**Backward (chain rule):**\n",
    "```\n",
    "dz2 = a2 - y                    # gradient at output\n",
    "dW2 = a1.T @ dz2 / n\n",
    "db2 = mean(dz2)\n",
    "\n",
    "da1 = dz2 @ W2.T                # backprop through weights\n",
    "dz1 = da1 * relu_derivative(z1) # backprop through activation\n",
    "dW1 = X.T @ dz1 / n\n",
    "db1 = mean(dz1)\n",
    "```\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"ReLU activation: max(0, z)\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU: 1 if z > 0, else 0\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "class TwoLayerMLP:\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        # Xavier initialization for weights\n",
    "        self.W1 = np.random.randn(n_input, n_hidden) * np.sqrt(2.0 / n_input)\n",
    "        self.b1 = np.zeros(n_hidden)\n",
    "        self.W2 = np.random.randn(n_hidden, n_output) * np.sqrt(2.0 / n_hidden)\n",
    "        self.b2 = np.zeros(n_output)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass - save intermediates for backprop.\n",
    "        X: (batch_size, n_input)\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1  # (batch, n_hidden)\n",
    "        self.a1 = relu(self.z1)                  # (batch, n_hidden)\n",
    "        \n",
    "        # Layer 2 (output)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2  # (batch, n_output)\n",
    "        self.a2 = sigmoid(self.z2)                     # (batch, n_output)\n",
    "        \n",
    "        return self.a2.squeeze()\n",
    "    \n",
    "    def backward(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Backward pass - compute all gradients via chain rule.\n",
    "        This is the heart of understanding neural networks!\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        y_true = y_true.reshape(-1, 1)  # (batch, 1)\n",
    "        \n",
    "        # ===== OUTPUT LAYER (Layer 2) =====\n",
    "        # dL/dz2 = a2 - y (same as single neuron)\n",
    "        dz2 = self.a2 - y_true  # (batch, 1)\n",
    "        \n",
    "        # dL/dW2 = a1^T @ dz2\n",
    "        self.dW2 = np.dot(self.a1.T, dz2) / n_samples  # (n_hidden, 1)\n",
    "        self.db2 = np.mean(dz2, axis=0)  # (1,)\n",
    "        \n",
    "        # ===== HIDDEN LAYER (Layer 1) =====\n",
    "        # Backprop through W2: da1 = dz2 @ W2^T\n",
    "        da1 = np.dot(dz2, self.W2.T)  # (batch, n_hidden)\n",
    "        \n",
    "        # Backprop through ReLU: dz1 = da1 * relu'(z1)\n",
    "        dz1 = da1 * relu_derivative(self.z1)  # (batch, n_hidden)\n",
    "        \n",
    "        # dL/dW1 = X^T @ dz1\n",
    "        self.dW1 = np.dot(X.T, dz1) / n_samples  # (n_input, n_hidden)\n",
    "        self.db1 = np.mean(dz1, axis=0)  # (n_hidden,)\n",
    "    \n",
    "    def update(self, lr):\n",
    "        \"\"\"Gradient descent update\"\"\"\n",
    "        self.W1 -= lr * self.dW1\n",
    "        self.b1 -= lr * self.db1\n",
    "        self.W2 -= lr * self.dW2\n",
    "        self.b2 -= lr * self.db2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.forward(X) >= 0.5).astype(int)\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        eps = 1e-15\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on XOR data\n",
    "mlp = TwoLayerMLP(n_input=2, n_hidden=8, n_output=1)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Forward\n",
    "    y_pred = mlp.forward(X_xor)\n",
    "    loss = mlp.compute_loss(y_xor, y_pred)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward\n",
    "    mlp.backward(X_xor, y_xor)\n",
    "    \n",
    "    # Update\n",
    "    mlp.update(lr=1.0)\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        acc = np.mean(mlp.predict(X_xor) == y_xor)\n",
    "        print(f\"Epoch {epoch}: Loss={loss:.4f}, Accuracy={acc:.2%}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('MLP Training on XOR')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the non-linear decision boundary\n",
    "plot_decision_boundary(mlp, X_xor, y_xor)\n",
    "print(f\"Final accuracy: {np.mean(mlp.predict(X_xor) == y_xor):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Activation Functions Deep Dive\n",
    "\n",
    "Different activations have different properties. Understanding these is crucial.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All common activation functions\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "def gelu(z):\n",
    "    \"\"\"Gaussian Error Linear Unit - used in transformers (BERT, GPT)\"\"\"\n",
    "    return 0.5 * z * (1 + np.tanh(np.sqrt(2 / np.pi) * (z + 0.044715 * z**3)))\n",
    "\n",
    "def swish(z):\n",
    "    \"\"\"Self-gated activation: z * sigmoid(z)\"\"\"\n",
    "    return z * sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all activations\n",
    "z = np.linspace(-4, 4, 200)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "activations = [\n",
    "    ('Sigmoid', sigmoid),\n",
    "    ('Tanh', tanh),\n",
    "    ('ReLU', relu),\n",
    "    ('Leaky ReLU', leaky_relu),\n",
    "    ('GELU', gelu),\n",
    "    ('Swish', swish)\n",
    "]\n",
    "\n",
    "for ax, (name, func) in zip(axes.flat, activations):\n",
    "    ax.plot(z, func(z), linewidth=2)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('z')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-4, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Different Activations?\n",
    "\n",
    "| Activation | Pros | Cons | Used In |\n",
    "|------------|------|------|--------|\n",
    "| Sigmoid | Bounded (0,1), interpretable as prob | Vanishing gradient, not zero-centered | Output layer (binary) |\n",
    "| Tanh | Zero-centered, bounded (-1,1) | Vanishing gradient | Old RNNs |\n",
    "| ReLU | Fast, no vanishing gradient for z>0 | Dead neurons (z<0 never recovers) | Most CNNs |\n",
    "| Leaky ReLU | No dead neurons | Extra hyperparameter | CNNs |\n",
    "| GELU | Smooth, probabilistic interpretation | Slower to compute | Transformers (BERT, GPT) |\n",
    "| Swish | Self-gated, smooth | Slower | Some modern nets |\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient Problem\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate vanishing gradients with sigmoid\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Simulate deep network: gradient gets multiplied at each layer\n",
    "# If derivative < 1, gradient shrinks exponentially\n",
    "\n",
    "z = np.linspace(-6, 6, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Sigmoid derivative is always < 0.25!\n",
    "axes[0].plot(z, sigmoid_derivative(z))\n",
    "axes[0].axhline(y=0.25, color='r', linestyle='--', label='max = 0.25')\n",
    "axes[0].set_title('Sigmoid Derivative')\n",
    "axes[0].set_xlabel('z')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# After n layers, gradient is multiplied by (derivative)^n\n",
    "n_layers = np.arange(1, 20)\n",
    "gradient_sigmoid = 0.25 ** n_layers  # worst case for sigmoid\n",
    "gradient_relu = 1.0 ** n_layers  # ReLU maintains gradient (when active)\n",
    "\n",
    "axes[1].semilogy(n_layers, gradient_sigmoid, 'b-', label='Sigmoid (worst case)')\n",
    "axes[1].semilogy(n_layers, gradient_relu, 'g-', label='ReLU (active)')\n",
    "axes[1].set_title('Gradient Magnitude vs Network Depth')\n",
    "axes[1].set_xlabel('Number of Layers')\n",
    "axes[1].set_ylabel('Gradient Scale (log)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"After 10 sigmoid layers, gradient is ~{0.25**10:.2e} of original\")\n",
    "print(\"This is why deep sigmoid networks don't train well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Weight Initialization\n",
    "\n",
    "Proper initialization is critical for training deep networks.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out):\n",
    "    \"\"\"\n",
    "    Xavier/Glorot initialization (for tanh/sigmoid).\n",
    "    Variance = 2 / (fan_in + fan_out)\n",
    "    \n",
    "    Goal: Keep variance of activations stable across layers.\n",
    "    \"\"\"\n",
    "    std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "    return np.random.randn(fan_in, fan_out) * std\n",
    "\n",
    "def kaiming_init(fan_in, fan_out):\n",
    "    \"\"\"\n",
    "    Kaiming/He initialization (for ReLU).\n",
    "    Variance = 2 / fan_in\n",
    "    \n",
    "    Accounts for ReLU zeroing out half the activations.\n",
    "    \"\"\"\n",
    "    std = np.sqrt(2.0 / fan_in)\n",
    "    return np.random.randn(fan_in, fan_out) * std\n",
    "\n",
    "def bad_init(fan_in, fan_out):\n",
    "    \"\"\"Too large - will cause exploding activations\"\"\"\n",
    "    return np.random.randn(fan_in, fan_out) * 1.0\n",
    "\n",
    "def tiny_init(fan_in, fan_out):\n",
    "    \"\"\"Too small - activations will vanish\"\"\"\n",
    "    return np.random.randn(fan_in, fan_out) * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate impact of initialization\n",
    "def simulate_forward_pass(init_func, n_layers=10, layer_size=256):\n",
    "    \"\"\"Simulate forward pass through many layers, track activation stats\"\"\"\n",
    "    x = np.random.randn(32, layer_size)  # batch of 32\n",
    "    activation_means = []\n",
    "    activation_stds = []\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        W = init_func(layer_size, layer_size)\n",
    "        x = relu(np.dot(x, W))\n",
    "        activation_means.append(np.mean(np.abs(x)))\n",
    "        activation_stds.append(np.std(x))\n",
    "    \n",
    "    return activation_means, activation_stds\n",
    "\n",
    "# Compare different initializations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for name, init_func in [('Xavier', xavier_init), ('Kaiming', kaiming_init), \n",
    "                         ('Too Large', bad_init), ('Too Small', tiny_init)]:\n",
    "    means, stds = simulate_forward_pass(init_func)\n",
    "    axes[0].plot(means, label=name)\n",
    "    axes[1].plot(stds, label=name)\n",
    "\n",
    "axes[0].set_title('Mean Activation Magnitude per Layer')\n",
    "axes[0].set_xlabel('Layer')\n",
    "axes[0].set_ylabel('Mean |activation|')\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].set_title('Activation Std Dev per Layer')\n",
    "axes[1].set_xlabel('Layer')\n",
    "axes[1].set_ylabel('Std Dev')\n",
    "axes[1].legend()\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Kaiming keeps activations stable with ReLU!\")\n",
    "print(\"Too large -> explodes, Too small -> vanishes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Batch Normalization vs Layer Normalization\n",
    "\n",
    "Normalization techniques stabilize training by controlling activation distributions.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm:\n",
    "    \"\"\"\n",
    "    Batch Normalization: normalize across the BATCH dimension.\n",
    "    \n",
    "    For input shape (batch_size, features):\n",
    "    - Compute mean/var across batch_size for each feature\n",
    "    - Each feature has its own mean/var\n",
    "    \n",
    "    Used in: CNNs, image models\n",
    "    Problem: Depends on batch size, doesn't work well for small batches or RNNs\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.gamma = np.ones(n_features)   # scale\n",
    "        self.beta = np.zeros(n_features)   # shift\n",
    "        \n",
    "        # Running statistics for inference\n",
    "        self.running_mean = np.zeros(n_features)\n",
    "        self.running_var = np.ones(n_features)\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        \"\"\"\n",
    "        x: (batch_size, n_features)\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            # Compute batch statistics\n",
    "            mean = np.mean(x, axis=0)  # mean over batch -> (n_features,)\n",
    "            var = np.var(x, axis=0)    # var over batch -> (n_features,)\n",
    "            \n",
    "            # Update running statistics\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / np.sqrt(var + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    \"\"\"\n",
    "    Layer Normalization: normalize across the FEATURE dimension.\n",
    "    \n",
    "    For input shape (batch_size, features):\n",
    "    - Compute mean/var across features for each sample\n",
    "    - Each sample has its own mean/var\n",
    "    \n",
    "    Used in: Transformers, RNNs\n",
    "    Advantage: Independent of batch size, works for any sequence length\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.gamma = np.ones(n_features)   # scale\n",
    "        self.beta = np.zeros(n_features)   # shift\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, n_features) or (batch_size, seq_len, n_features)\n",
    "        \"\"\"\n",
    "        # Compute statistics across last dimension (features)\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)  # (batch,) or (batch, seq, 1)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / np.sqrt(var + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference\n",
    "batch_size = 4\n",
    "n_features = 8\n",
    "\n",
    "# Create sample data with different scales per feature\n",
    "x = np.random.randn(batch_size, n_features)\n",
    "x[:, 0] *= 10  # feature 0 has large scale\n",
    "x[:, 1] *= 0.1  # feature 1 has small scale\n",
    "\n",
    "bn = BatchNorm(n_features)\n",
    "ln = LayerNorm(n_features)\n",
    "\n",
    "x_bn = bn.forward(x)\n",
    "x_ln = ln.forward(x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Original\n",
    "im0 = axes[0].imshow(x, aspect='auto', cmap='RdBu', vmin=-3, vmax=3)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].set_xlabel('Features')\n",
    "axes[0].set_ylabel('Batch Samples')\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "# BatchNorm\n",
    "im1 = axes[1].imshow(x_bn, aspect='auto', cmap='RdBu', vmin=-3, vmax=3)\n",
    "axes[1].set_title('After BatchNorm\\n(normalize per COLUMN)')\n",
    "axes[1].set_xlabel('Features')\n",
    "axes[1].set_ylabel('Batch Samples')\n",
    "plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "# LayerNorm\n",
    "im2 = axes[2].imshow(x_ln, aspect='auto', cmap='RdBu', vmin=-3, vmax=3)\n",
    "axes[2].set_title('After LayerNorm\\n(normalize per ROW)')\n",
    "axes[2].set_xlabel('Features')\n",
    "axes[2].set_ylabel('Batch Samples')\n",
    "plt.colorbar(im2, ax=axes[2])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"BatchNorm: Each FEATURE (column) has mean~0, var~1\")\n",
    "print(f\"  Feature means: {x_bn.mean(axis=0).round(2)}\")\n",
    "print(f\"\\nLayerNorm: Each SAMPLE (row) has mean~0, var~1\")\n",
    "print(f\"  Sample means: {x_ln.mean(axis=1).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use which?\n",
    "\n",
    "| Normalization | Normalize Across | Best For | Why |\n",
    "|--------------|------------------|----------|-----|\n",
    "| BatchNorm | Batch (samples) | CNNs, fixed input size | Features should have consistent scale |\n",
    "| LayerNorm | Features | Transformers, RNNs | Batch-independent, variable sequence length |\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "1. **Single neuron** = linear decision boundary, can only solve linearly separable problems\n",
    "2. **Hidden layers** = can learn non-linear boundaries (universal approximation)\n",
    "3. **Backpropagation** = chain rule applied systematically through the network\n",
    "4. **Activation functions** matter: ReLU for depth, GELU for transformers\n",
    "5. **Initialization** must preserve signal variance (Xavier/Kaiming)\n",
    "6. **Normalization** stabilizes training (BatchNorm for CNNs, LayerNorm for transformers)\n",
    "\n",
    "**Your notes here:**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
