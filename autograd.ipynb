{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Autograd from Scratch\n",
    "\n",
    "Understanding automatic differentiation - the core of PyTorch, TensorFlow, JAX.\n",
    "\n",
    "**Goal**: Build a mini version of PyTorch's autograd system.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: The Core Idea\n",
    "\n",
    "**Manual differentiation** (what we did in nn.ipynb): \n",
    "- Write forward pass\n",
    "- Manually derive gradients\n",
    "- Write backward pass\n",
    "\n",
    "**Automatic differentiation**:\n",
    "- Write forward pass only\n",
    "- System automatically computes gradients!\n",
    "\n",
    "**How?** Build a computational graph and use the chain rule.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Computational Graph\n",
    "\n",
    "```python\n",
    "# Forward\n",
    "a = 2.0\n",
    "b = 3.0\n",
    "c = a + b      # c = 5\n",
    "d = c * a      # d = 10\n",
    "e = c * d      # e = 50\n",
    "```\n",
    "\n",
    "Graph:\n",
    "```\n",
    "  a(2) ──┬──> c(5) ──┬──> e(50)\n",
    "         │           │\n",
    "  b(3) ──┘     d(10)─┘\n",
    "         │       │\n",
    "  a(2) ──────────┘\n",
    "```\n",
    "\n",
    "**Backward** (chain rule):\n",
    "```\n",
    "de/de = 1\n",
    "de/dd = c = 5\n",
    "de/dc = d = 10\n",
    "de/da = de/dc * dc/da + de/dd * dd/da = 10*1 + 5*c = 10 + 25 = 35\n",
    "de/db = de/dc * dc/db = 10 * 1 = 10\n",
    "```\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: The Value Class\n",
    "\n",
    "Each value knows:\n",
    "1. Its data (the number)\n",
    "2. Its gradient (∂L/∂value)\n",
    "3. Its parents (what created it)\n",
    "4. How to backprop (backward function)\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\"\n",
    "    A node in the computational graph.\n",
    "    \n",
    "    Stores:\n",
    "    - data: the actual value\n",
    "    - grad: gradient of some loss w.r.t. this value\n",
    "    - _backward: function to propagate gradients to children\n",
    "    - _prev: parent nodes (what operations created this)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0  # gradient starts at 0\n",
    "        self._backward = lambda: None  # default: do nothing\n",
    "        self._prev = set(_children)  # parent nodes\n",
    "        self._op = _op  # operation that created this (for visualization)\n",
    "        self.label = label\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"Addition: z = x + y\"\"\"\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            # dz/dx = 1, dz/dy = 1\n",
    "            # Chain rule: dL/dx += dL/dz * dz/dx = dL/dz * 1\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Multiplication: z = x * y\"\"\"\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            # dz/dx = y, dz/dy = x\n",
    "            # Chain rule: dL/dx += dL/dz * dz/dx\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        \"\"\"Power: z = x^k\"\"\"\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "        \n",
    "        def _backward():\n",
    "            # dz/dx = k * x^(k-1)\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        \"\"\"Hyperbolic tangent activation\"\"\"\n",
    "        x = self.data\n",
    "        t = (np.exp(2*x) - 1) / (np.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            # d(tanh(x))/dx = 1 - tanh^2(x)\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        \"\"\"Exponential\"\"\"\n",
    "        x = self.data\n",
    "        out = Value(np.exp(x), (self,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            # d(e^x)/dx = e^x\n",
    "            self.grad += out.data * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "    \n",
    "    def __radd__(self, other):  # other + self\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __rsub__(self, other):  # other - self\n",
    "        return other + (-self)\n",
    "    \n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other\n",
    "    \n",
    "    def __truediv__(self, other):  # self / other\n",
    "        return self * other**-1\n",
    "    \n",
    "    def __rtruediv__(self, other):  # other / self\n",
    "        return other * self**-1\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Run backpropagation from this node.\n",
    "        \n",
    "        Topological sort ensures we visit nodes in the right order\n",
    "        (children before parents).\n",
    "        \"\"\"\n",
    "        # Build topological order\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        \n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        \n",
    "        build_topo(self)\n",
    "        \n",
    "        # Start backprop: gradient of self w.r.t. itself is 1\n",
    "        self.grad = 1.0\n",
    "        \n",
    "        # Apply chain rule in reverse topological order\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "print(\"Value class created! Let's test it...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Testing Basic Operations\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from earlier\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(3.0, label='b')\n",
    "c = a + b\n",
    "c.label = 'c'\n",
    "d = c * a\n",
    "d.label = 'd'\n",
    "e = c * d\n",
    "e.label = 'e'\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"a = {a.data}\")\n",
    "print(f\"b = {b.data}\")\n",
    "print(f\"c = a + b = {c.data}\")\n",
    "print(f\"d = c * a = {d.data}\")\n",
    "print(f\"e = c * d = {e.data}\")\n",
    "\n",
    "# Backward pass\n",
    "e.backward()\n",
    "\n",
    "print(\"\\nBackward pass (gradients):\")\n",
    "print(f\"de/de = {e.grad}\")\n",
    "print(f\"de/dd = {d.grad}\")\n",
    "print(f\"de/dc = {c.grad}\")\n",
    "print(f\"de/da = {a.grad}  (expected: 35)\")\n",
    "print(f\"de/db = {b.grad}  (expected: 10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex example with activation\n",
    "x = Value(0.5, label='x')\n",
    "y = Value(-1.0, label='y')\n",
    "\n",
    "z = x**2 + y**2  # z = 0.25 + 1.0 = 1.25\n",
    "z.label = 'z'\n",
    "\n",
    "w = z.tanh()  # w = tanh(1.25) ≈ 0.848\n",
    "w.label = 'w'\n",
    "\n",
    "print(f\"z = x^2 + y^2 = {z.data:.4f}\")\n",
    "print(f\"w = tanh(z) = {w.data:.4f}\")\n",
    "\n",
    "w.backward()\n",
    "\n",
    "print(f\"\\ndw/dx = {x.grad:.4f}\")\n",
    "print(f\"dw/dy = {y.grad:.4f}\")\n",
    "\n",
    "# Verify with numerical gradient\n",
    "h = 1e-6\n",
    "x_val = 0.5\n",
    "y_val = -1.0\n",
    "\n",
    "def f(x, y):\n",
    "    z = x**2 + y**2\n",
    "    return np.tanh(z)\n",
    "\n",
    "numerical_dx = (f(x_val + h, y_val) - f(x_val - h, y_val)) / (2*h)\n",
    "numerical_dy = (f(x_val, y_val + h) - f(x_val, y_val - h)) / (2*h)\n",
    "\n",
    "print(f\"\\nNumerical verification:\")\n",
    "print(f\"dw/dx (numerical) = {numerical_dx:.4f}\")\n",
    "print(f\"dw/dy (numerical) = {numerical_dy:.4f}\")\n",
    "print(\"\\nMatches! ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Building a Neural Network with Autograd\n",
    "\n",
    "Now let's use our autograd to build a real neural network!\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    A single neuron: y = tanh(w·x + b)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs):\n",
    "        self.w = [Value(np.random.randn()) for _ in range(n_inputs)]\n",
    "        self.b = Value(np.random.randn())\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # w·x + b\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    A layer of neurons.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.neurons = [Neuron(n_inputs) for _ in range(n_outputs)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outs = [neuron(x) for neuron in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, layer_sizes):\n",
    "        sz = [n_inputs] + layer_sizes\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(layer_sizes))]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# Create a 2-layer MLP: 3 inputs -> 4 hidden -> 1 output\n",
    "mlp = MLP(3, [4, 1])\n",
    "print(f\"MLP with {len(mlp.parameters())} parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "x = [Value(1.0), Value(-0.5), Value(0.3)]\n",
    "y = mlp(x)\n",
    "print(f\"\\nOutput: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the MLP\n",
    "\n",
    "Let's train it to learn a simple function.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: simple XOR-like problem\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]  # targets\n",
    "\n",
    "# Convert to Value objects\n",
    "xs = [[Value(x) for x in row] for row in xs]\n",
    "ys = [Value(y) for y in ys]\n",
    "\n",
    "# Initialize MLP\n",
    "np.random.seed(42)\n",
    "mlp = MLP(3, [4, 4, 1])\n",
    "\n",
    "print(f\"Training MLP with {len(mlp.parameters())} parameters\")\n",
    "print(f\"Training set: {len(xs)} examples\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "losses = []\n",
    "learning_rate = 0.05\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    ypred = [mlp(x) for x in xs]\n",
    "    \n",
    "    # Loss: mean squared error\n",
    "    loss = sum((yp - yt)**2 for yt, yp in zip(ys, ypred))\n",
    "    \n",
    "    # Backward pass\n",
    "    # Zero gradients\n",
    "    for p in mlp.parameters():\n",
    "        p.grad = 0.0\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    for p in mlp.parameters():\n",
    "        p.data += -learning_rate * p.grad\n",
    "    \n",
    "    losses.append(loss.data)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss = {loss.data:.6f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (Our Autograd MLP)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "print(\"\\nFinal predictions:\")\n",
    "print(\"-\" * 40)\n",
    "for i, (x, y_true) in enumerate(zip(xs, ys)):\n",
    "    y_pred = mlp(x)\n",
    "    print(f\"Example {i}: Target={y_true.data:+.1f}, Predicted={y_pred.data:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Comparison with PyTorch\n",
    "\n",
    "Let's verify our implementation matches PyTorch.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Same computation with PyTorch\n",
    "x1 = torch.tensor([2.0], requires_grad=True)\n",
    "x2 = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# Forward\n",
    "y = x1**2 + x2**2\n",
    "z = torch.tanh(y)\n",
    "\n",
    "# Backward\n",
    "z.backward()\n",
    "\n",
    "print(\"PyTorch:\")\n",
    "print(f\"z = {z.item():.4f}\")\n",
    "print(f\"dz/dx1 = {x1.grad.item():.4f}\")\n",
    "print(f\"dz/dx2 = {x2.grad.item():.4f}\")\n",
    "\n",
    "# Our implementation\n",
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = a**2 + b**2\n",
    "d = c.tanh()\n",
    "d.backward()\n",
    "\n",
    "print(\"\\nOur Autograd:\")\n",
    "print(f\"d = {d.data:.4f}\")\n",
    "print(f\"dd/da = {a.grad:.4f}\")\n",
    "print(f\"dd/db = {b.grad:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Matches perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Visualizing the Computational Graph\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from graphviz import Digraph\n",
    "    \n",
    "    def trace(root):\n",
    "        nodes, edges = set(), set()\n",
    "        def build(v):\n",
    "            if v not in nodes:\n",
    "                nodes.add(v)\n",
    "                for child in v._prev:\n",
    "                    edges.add((child, v))\n",
    "                    build(child)\n",
    "        build(root)\n",
    "        return nodes, edges\n",
    "    \n",
    "    def draw_dot(root, format='svg', rankdir='LR'):\n",
    "        \"\"\"\n",
    "        Draw computational graph.\n",
    "        format: 'svg', 'png'\n",
    "        rankdir: 'LR' (left to right), 'TB' (top to bottom)\n",
    "        \"\"\"\n",
    "        assert rankdir in ['LR', 'TB']\n",
    "        nodes, edges = trace(root)\n",
    "        dot = Digraph(format=format, graph_attr={'rankdir': rankdir})\n",
    "        \n",
    "        for n in nodes:\n",
    "            dot.node(name=str(id(n)), \n",
    "                    label=f\"{n.label} | data {n.data:.2f} | grad {n.grad:.2f}\",\n",
    "                    shape='record')\n",
    "            if n._op:\n",
    "                dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "                dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "        \n",
    "        for n1, n2 in edges:\n",
    "            dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "        \n",
    "        return dot\n",
    "    \n",
    "    # Draw example graph\n",
    "    x = Value(2.0, label='x')\n",
    "    y = Value(3.0, label='y')\n",
    "    z = x * y\n",
    "    z.label = 'z'\n",
    "    w = z + x\n",
    "    w.label = 'w'\n",
    "    w.backward()\n",
    "    \n",
    "    graph = draw_dot(w)\n",
    "    graph.render('comp_graph', view=False, cleanup=True)\n",
    "    print(\"Computational graph saved as 'comp_graph.svg'\")\n",
    "    \nexcept ImportError:\n",
    "    print(\"Install graphviz to visualize: pip install graphviz\")\n",
    "    print(\"Skipping visualization...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Understanding How PyTorch Works\n",
    "\n",
    "Our mini autograd is essentially what PyTorch does, but:\n",
    "\n",
    "1. **PyTorch operates on tensors** (multi-dimensional arrays)\n",
    "2. **GPU acceleration** via CUDA\n",
    "3. **Optimized C++/CUDA kernels** for operations\n",
    "4. **Memory management** (in-place ops, gradient checkpointing)\n",
    "5. **More operations** (convolution, pooling, attention, etc.)\n",
    "\n",
    "But the **core idea is identical**:\n",
    "- Build computational graph\n",
    "- Store backward functions\n",
    "- Apply chain rule in reverse topological order\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts to Remember\n",
    "\n",
    "1. **Forward pass**: Build the computational graph\n",
    "2. **Backward pass**: Apply chain rule automatically\n",
    "3. **Topological sort**: Visit nodes in the right order\n",
    "4. **Gradient accumulation**: `+=` not `=` (multiple paths to same node)\n",
    "5. **Zero gradients**: Reset before each backward pass\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Extension Ideas\n",
    "\n",
    "To make this a real autograd engine, add:\n",
    "\n",
    "1. **Tensor support** (extend to n-dimensional arrays)\n",
    "2. **Broadcasting** (handle different shapes)\n",
    "3. **More operations**:\n",
    "   - Matrix multiplication\n",
    "   - Convolution\n",
    "   - Softmax\n",
    "   - Layer normalization\n",
    "4. **In-place operations** (memory efficiency)\n",
    "5. **GPU support** (CUDA kernels)\n",
    "6. **Static vs dynamic graphs** (TensorFlow vs PyTorch)\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You've built a working autograd engine! You now understand:\n",
    "\n",
    "1. **Computational graphs**: Nodes are values, edges are operations\n",
    "2. **Automatic differentiation**: Chain rule applied systematically\n",
    "3. **Backpropagation**: Traverse graph backwards, accumulate gradients\n",
    "4. **How PyTorch/TensorFlow work** under the hood\n",
    "\n",
    "This is the **foundation of all modern deep learning frameworks**.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. Add a `relu()` method to the `Value` class\n",
    "2. Add a `sigmoid()` method\n",
    "3. Implement matrix multiplication for the `Value` class\n",
    "4. Build a Conv2D operation (harder!)\n",
    "5. Extend `Value` to support n-dimensional arrays (become a mini numpy with autograd)\n",
    "\n",
    "**Your notes here:**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
