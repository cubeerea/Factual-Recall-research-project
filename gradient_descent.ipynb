{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent & Optimization Algorithms\n",
    "\n",
    "Understanding optimization from first principles.\n",
    "\n",
    "**Goal**: Know how neural networks actually learn - the mechanics of updating weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Vanilla Gradient Descent\n",
    "\n",
    "The fundamental algorithm: follow the negative gradient downhill.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2D function to minimize: f(x, y) = x^2 + y^2\n",
    "# Global minimum at (0, 0)\n",
    "\n",
    "def simple_bowl(x, y):\n",
    "    \"\"\"Simple convex function - easy to optimize\"\"\"\n",
    "    return x**2 + y**2\n",
    "\n",
    "def simple_bowl_grad(x, y):\n",
    "    \"\"\"Gradient of simple_bowl\"\"\"\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "# Visualize the function\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = simple_bowl(X, Y)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.8)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x, y)')\n",
    "ax1.set_title('3D Surface')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(X, Y, Z, levels=20)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Contour Plot')\n",
    "ax2.plot(0, 0, 'r*', markersize=15, label='Global minimum')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Gradient Descent\n",
    "\n",
    "Update rule:\n",
    "```\n",
    "θ_new = θ_old - learning_rate * ∇f(θ_old)\n",
    "```\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_gradient_descent(grad_func, start_pos, lr=0.1, n_iters=50):\n",
    "    \"\"\"\n",
    "    Vanilla GD: just follow the negative gradient.\n",
    "    \n",
    "    Args:\n",
    "        grad_func: function that returns gradient at a point\n",
    "        start_pos: initial position (numpy array)\n",
    "        lr: learning rate\n",
    "        n_iters: number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        trajectory: list of positions visited\n",
    "    \"\"\"\n",
    "    pos = start_pos.copy()\n",
    "    trajectory = [pos.copy()]\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        grad = grad_func(pos[0], pos[1])\n",
    "        pos = pos - lr * grad\n",
    "        trajectory.append(pos.copy())\n",
    "    \n",
    "    return np.array(trajectory)\n",
    "\n",
    "# Run vanilla GD from point (4, 4)\n",
    "start = np.array([4.0, 4.0])\n",
    "traj = vanilla_gradient_descent(simple_bowl_grad, start, lr=0.1, n_iters=20)\n",
    "\n",
    "# Visualize the path\n",
    "plt.figure(figsize=(8, 8))\n",
    "contour = plt.contour(X, Y, Z, levels=20, alpha=0.5)\n",
    "plt.plot(traj[:, 0], traj[:, 1], 'ro-', linewidth=2, markersize=8, label='GD path')\n",
    "plt.plot(traj[0, 0], traj[0, 1], 'go', markersize=12, label='Start')\n",
    "plt.plot(0, 0, 'r*', markersize=15, label='Global minimum')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Vanilla Gradient Descent Path')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Started at: {traj[0]}\")\n",
    "print(f\"Ended at: {traj[-1]}\")\n",
    "print(f\"Distance from minimum: {np.linalg.norm(traj[-1]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Sensitivity\n",
    "\n",
    "Learning rate is the most important hyperparameter.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.05, 0.3, 0.9, 1.1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "for ax, lr in zip(axes.flat, learning_rates):\n",
    "    try:\n",
    "        traj = vanilla_gradient_descent(simple_bowl_grad, start, lr=lr, n_iters=20)\n",
    "        \n",
    "        ax.contour(X, Y, Z, levels=20, alpha=0.5)\n",
    "        ax.plot(traj[:, 0], traj[:, 1], 'ro-', linewidth=2, markersize=6)\n",
    "        ax.plot(traj[0, 0], traj[0, 1], 'go', markersize=10)\n",
    "        ax.plot(0, 0, 'r*', markersize=15)\n",
    "        ax.set_xlim(-5, 5)\n",
    "        ax.set_ylim(-5, 5)\n",
    "        ax.set_title(f'Learning Rate = {lr}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        if lr >= 1.0:\n",
    "            ax.text(0, 4.5, 'DIVERGES!', fontsize=14, color='red', \n",
    "                   ha='center', weight='bold')\n",
    "    except:\n",
    "        ax.text(0.5, 0.5, f'LR={lr}\\nDIVERGED', transform=ax.transAxes,\n",
    "               fontsize=16, ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Too small: slow convergence\")\n",
    "print(\"Too large: oscillates or diverges\")\n",
    "print(\"Just right: smooth, fast convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Batch vs Stochastic vs Mini-Batch GD\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple linear regression dataset\n",
    "def generate_regression_data(n_samples=100):\n",
    "    X = np.random.randn(n_samples, 1) * 2\n",
    "    y = 3 * X.squeeze() + 7 + np.random.randn(n_samples) * 0.5\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate_regression_data(100)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train, y_train, alpha=0.6)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Regression Data: y = 3x + 7 + noise')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"Mean squared error\"\"\"\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Gradient of MSE loss for linear regression.\n",
    "    \n",
    "    y_pred = X @ w + b\n",
    "    loss = mean((y - y_pred)^2)\n",
    "    \n",
    "    dL/dw = -2 * mean(X * (y - y_pred))\n",
    "    dL/db = -2 * mean(y - y_pred)\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    y_pred = X @ w + b\n",
    "    error = y - y_pred.squeeze()\n",
    "    \n",
    "    dw = -2 * np.mean(X * error.reshape(-1, 1), axis=0)\n",
    "    db = -2 * np.mean(error)\n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent\n",
    "\n",
    "Uses ALL data to compute gradient every step.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, y, lr=0.01, n_epochs=100):\n",
    "    \"\"\"\n",
    "    Batch GD: compute gradient using ALL samples every iteration.\n",
    "    \n",
    "    + Smooth, stable updates\n",
    "    - Slow for large datasets (must process all data for one update)\n",
    "    \"\"\"\n",
    "    w = np.random.randn(1)\n",
    "    b = 0.0\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Compute gradient using ALL data\n",
    "        dw, db = compute_gradient(X, y, w, b)\n",
    "        \n",
    "        # Update\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "        \n",
    "        # Track loss\n",
    "        y_pred = X @ w + b\n",
    "        loss = mse_loss(y, y_pred.squeeze())\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return w, b, losses\n",
    "\n",
    "w_batch, b_batch, losses_batch = batch_gd(X_train, y_train, lr=0.1, n_epochs=100)\n",
    "\n",
    "print(f\"True parameters: w=3, b=7\")\n",
    "print(f\"Learned (Batch GD): w={w_batch[0]:.3f}, b={b_batch:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Uses ONE random sample to compute gradient.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gd(X, y, lr=0.01, n_epochs=100):\n",
    "    \"\"\"\n",
    "    Stochastic GD: compute gradient using ONE random sample.\n",
    "    \n",
    "    + Can escape local minima (noisy updates)\n",
    "    + Faster iterations\n",
    "    - Very noisy, can be unstable\n",
    "    \"\"\"\n",
    "    w = np.random.randn(1)\n",
    "    b = 0.0\n",
    "    losses = []\n",
    "    \n",
    "    n = len(y)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle data each epoch\n",
    "        indices = np.random.permutation(n)\n",
    "        \n",
    "        for i in indices:\n",
    "            # Compute gradient using SINGLE sample\n",
    "            Xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            dw, db = compute_gradient(Xi, yi, w, b)\n",
    "            \n",
    "            # Update\n",
    "            w -= lr * dw\n",
    "            b -= lr * db\n",
    "        \n",
    "        # Track loss on full dataset\n",
    "        y_pred = X @ w + b\n",
    "        loss = mse_loss(y, y_pred.squeeze())\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return w, b, losses\n",
    "\n",
    "w_sgd, b_sgd, losses_sgd = stochastic_gd(X_train, y_train, lr=0.01, n_epochs=100)\n",
    "\n",
    "print(f\"True parameters: w=3, b=7\")\n",
    "print(f\"Learned (SGD): w={w_sgd[0]:.3f}, b={b_sgd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "Best of both worlds: use a small batch of samples.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gd(X, y, lr=0.01, n_epochs=100, batch_size=16):\n",
    "    \"\"\"\n",
    "    Mini-batch GD: compute gradient using a BATCH of samples.\n",
    "    \n",
    "    + Balance between stability and speed\n",
    "    + Efficient on GPUs (parallel computation)\n",
    "    + Default in practice (batch_size typically 32-256)\n",
    "    \"\"\"\n",
    "    w = np.random.randn(1)\n",
    "    b = 0.0\n",
    "    losses = []\n",
    "    \n",
    "    n = len(y)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(n)\n",
    "        \n",
    "        # Process in mini-batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            batch_indices = indices[start:end]\n",
    "            \n",
    "            # Compute gradient on mini-batch\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            dw, db = compute_gradient(X_batch, y_batch, w, b)\n",
    "            \n",
    "            # Update\n",
    "            w -= lr * dw\n",
    "            b -= lr * db\n",
    "        \n",
    "        # Track loss\n",
    "        y_pred = X @ w + b\n",
    "        loss = mse_loss(y, y_pred.squeeze())\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return w, b, losses\n",
    "\n",
    "w_mini, b_mini, losses_mini = minibatch_gd(X_train, y_train, lr=0.1, \n",
    "                                            n_epochs=100, batch_size=16)\n",
    "\n",
    "print(f\"True parameters: w=3, b=7\")\n",
    "print(f\"Learned (Mini-batch): w={w_mini[0]:.3f}, b={b_mini:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses_batch, label='Batch GD', linewidth=2)\n",
    "plt.plot(losses_sgd, label='SGD', alpha=0.7)\n",
    "plt.plot(losses_mini, label='Mini-batch GD', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Batch vs SGD vs Mini-Batch Gradient Descent')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice:\")\n",
    "print(\"- Batch GD: smooth but may converge slowly\")\n",
    "print(\"- SGD: noisy but can escape bad regions\")\n",
    "print(\"- Mini-batch: best trade-off (used in practice)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Momentum\n",
    "\n",
    "Momentum accelerates GD by accumulating velocity.\n",
    "\n",
    "**Analogy**: Ball rolling down a hill builds up speed.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_gd(grad_func, start_pos, lr=0.01, momentum=0.9, n_iters=50):\n",
    "    \"\"\"\n",
    "    Momentum GD: accumulate velocity.\n",
    "    \n",
    "    v_t = β * v_{t-1} + ∇f(θ)\n",
    "    θ_t = θ_{t-1} - lr * v_t\n",
    "    \n",
    "    β (momentum coefficient): typically 0.9\n",
    "    - Higher β = more \"inertia\", smoother path\n",
    "    - Lower β = less memory of past gradients\n",
    "    \"\"\"\n",
    "    pos = start_pos.copy()\n",
    "    velocity = np.zeros_like(pos)\n",
    "    trajectory = [pos.copy()]\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        grad = grad_func(pos[0], pos[1])\n",
    "        \n",
    "        # Update velocity (accumulate past gradients)\n",
    "        velocity = momentum * velocity + grad\n",
    "        \n",
    "        # Update position using velocity\n",
    "        pos = pos - lr * velocity\n",
    "        trajectory.append(pos.copy())\n",
    "    \n",
    "    return np.array(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare vanilla GD vs Momentum on a harder function\n",
    "def ravine_func(x, y):\n",
    "    \"\"\"Ravine function - steep in one direction, shallow in another\"\"\"\n",
    "    return x**2 + 10*y**2\n",
    "\n",
    "def ravine_grad(x, y):\n",
    "    return np.array([2*x, 20*y])\n",
    "\n",
    "# Visualize ravine\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = ravine_func(X, Y)\n",
    "\n",
    "start = np.array([8.0, 3.0])\n",
    "\n",
    "traj_vanilla = vanilla_gradient_descent(ravine_grad, start, lr=0.05, n_iters=50)\n",
    "traj_momentum = momentum_gd(ravine_grad, start, lr=0.05, momentum=0.9, n_iters=50)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.contour(X, Y, Z, levels=30, alpha=0.5)\n",
    "plt.plot(traj_vanilla[:, 0], traj_vanilla[:, 1], 'ro-', label='Vanilla GD', markersize=4)\n",
    "plt.plot(start[0], start[1], 'go', markersize=10)\n",
    "plt.plot(0, 0, 'r*', markersize=15)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Vanilla GD on Ravine (oscillates!)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.contour(X, Y, Z, levels=30, alpha=0.5)\n",
    "plt.plot(traj_momentum[:, 0], traj_momentum[:, 1], 'bo-', label='Momentum', markersize=4)\n",
    "plt.plot(start[0], start[1], 'go', markersize=10)\n",
    "plt.plot(0, 0, 'r*', markersize=15)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Momentum GD (smoother path!)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Vanilla GD final distance: {np.linalg.norm(traj_vanilla[-1]):.4f}\")\n",
    "print(f\"Momentum GD final distance: {np.linalg.norm(traj_momentum[-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: RMSprop\n",
    "\n",
    "RMSprop: Adaptive learning rates per parameter.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop(grad_func, start_pos, lr=0.1, decay=0.9, eps=1e-8, n_iters=50):\n",
    "    \"\"\"\n",
    "    RMSprop: Root Mean Square Propagation.\n",
    "    \n",
    "    Adapts learning rate for each parameter based on recent gradients.\n",
    "    \n",
    "    v_t = β * v_{t-1} + (1-β) * (∇f)^2    (running avg of squared gradients)\n",
    "    θ_t = θ_{t-1} - (lr / sqrt(v_t + ε)) * ∇f\n",
    "    \n",
    "    Intuition: If a parameter has large gradients, divide by large number (slower updates)\n",
    "               If small gradients, divide by small number (faster updates)\n",
    "    \"\"\"\n",
    "    pos = start_pos.copy()\n",
    "    v = np.zeros_like(pos)  # running average of squared gradients\n",
    "    trajectory = [pos.copy()]\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        grad = grad_func(pos[0], pos[1])\n",
    "        \n",
    "        # Update running average of squared gradients\n",
    "        v = decay * v + (1 - decay) * grad**2\n",
    "        \n",
    "        # Adaptive learning rate per parameter\n",
    "        pos = pos - (lr / (np.sqrt(v) + eps)) * grad\n",
    "        trajectory.append(pos.copy())\n",
    "    \n",
    "    return np.array(trajectory)\n",
    "\n",
    "traj_rmsprop = rmsprop(ravine_grad, start, lr=0.5, decay=0.9, n_iters=50)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.contour(X, Y, Z, levels=30, alpha=0.5)\n",
    "plt.plot(traj_rmsprop[:, 0], traj_rmsprop[:, 1], 'mo-', label='RMSprop', markersize=4)\n",
    "plt.plot(start[0], start[1], 'go', markersize=10)\n",
    "plt.plot(0, 0, 'r*', markersize=15)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('RMSprop (adapts to geometry!)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"RMSprop final distance: {np.linalg.norm(traj_rmsprop[-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Adam (The King)\n",
    "\n",
    "Adam = Momentum + RMSprop. Default optimizer for most problems.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(grad_func, start_pos, lr=0.1, beta1=0.9, beta2=0.999, eps=1e-8, n_iters=50):\n",
    "    \"\"\"\n",
    "    Adam: Adaptive Moment Estimation.\n",
    "    \n",
    "    Combines:\n",
    "    - Momentum (first moment of gradients)\n",
    "    - RMSprop (second moment of gradients)\n",
    "    \n",
    "    m_t = β1 * m_{t-1} + (1-β1) * ∇f        (momentum)\n",
    "    v_t = β2 * v_{t-1} + (1-β2) * (∇f)^2    (RMSprop)\n",
    "    \n",
    "    # Bias correction (important for early iterations)\n",
    "    m_hat = m_t / (1 - β1^t)\n",
    "    v_hat = v_t / (1 - β2^t)\n",
    "    \n",
    "    θ_t = θ_{t-1} - (lr / sqrt(v_hat) + ε) * m_hat\n",
    "    \n",
    "    Default values: β1=0.9, β2=0.999, lr=0.001\n",
    "    \"\"\"\n",
    "    pos = start_pos.copy()\n",
    "    m = np.zeros_like(pos)  # first moment (momentum)\n",
    "    v = np.zeros_like(pos)  # second moment (RMSprop)\n",
    "    trajectory = [pos.copy()]\n",
    "    \n",
    "    for t in range(1, n_iters + 1):\n",
    "        grad = grad_func(pos[0], pos[1])\n",
    "        \n",
    "        # Update biased first moment\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        \n",
    "        # Update biased second moment\n",
    "        v = beta2 * v + (1 - beta2) * grad**2\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "        \n",
    "        # Update parameters\n",
    "        pos = pos - (lr / (np.sqrt(v_hat) + eps)) * m_hat\n",
    "        trajectory.append(pos.copy())\n",
    "    \n",
    "    return np.array(trajectory)\n",
    "\n",
    "traj_adam = adam(ravine_grad, start, lr=0.5, n_iters=50)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.contour(X, Y, Z, levels=30, alpha=0.5)\n",
    "plt.plot(traj_adam[:, 0], traj_adam[:, 1], 'co-', label='Adam', markersize=4)\n",
    "plt.plot(start[0], start[1], 'go', markersize=10)\n",
    "plt.plot(0, 0, 'r*', markersize=15)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Adam (smooth + adaptive!)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Adam final distance: {np.linalg.norm(traj_adam[-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare All Optimizers\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all optimizers\n",
    "start = np.array([8.0, 3.0])\n",
    "\n",
    "optimizers = [\n",
    "    ('Vanilla GD', vanilla_gradient_descent(ravine_grad, start, lr=0.05, n_iters=50), 'red'),\n",
    "    ('Momentum', momentum_gd(ravine_grad, start, lr=0.05, momentum=0.9, n_iters=50), 'blue'),\n",
    "    ('RMSprop', rmsprop(ravine_grad, start, lr=0.5, n_iters=50), 'magenta'),\n",
    "    ('Adam', adam(ravine_grad, start, lr=0.5, n_iters=50), 'cyan'),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.contour(X, Y, Z, levels=40, alpha=0.3)\n",
    "\n",
    "for name, traj, color in optimizers:\n",
    "    plt.plot(traj[:, 0], traj[:, 1], 'o-', color=color, label=name, \n",
    "             markersize=3, linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.plot(start[0], start[1], 'go', markersize=15, label='Start', zorder=5)\n",
    "plt.plot(0, 0, 'r*', markersize=20, label='Global min', zorder=5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Optimizer Comparison on Ravine Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal distances from minimum:\")\n",
    "for name, traj, _ in optimizers:\n",
    "    dist = np.linalg.norm(traj[-1])\n",
    "    print(f\"{name:15s}: {dist:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Learning Rate Schedules\n",
    "\n",
    "Adjusting learning rate during training.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_lr(initial_lr, epoch, total_epochs):\n",
    "    \"\"\"Constant learning rate\"\"\"\n",
    "    return initial_lr\n",
    "\n",
    "def step_decay(initial_lr, epoch, total_epochs, drop=0.5, epochs_drop=10):\n",
    "    \"\"\"Drop by factor every N epochs\"\"\"\n",
    "    return initial_lr * (drop ** (epoch // epochs_drop))\n",
    "\n",
    "def exponential_decay(initial_lr, epoch, total_epochs, decay_rate=0.95):\n",
    "    \"\"\"Exponential decay\"\"\"\n",
    "    return initial_lr * (decay_rate ** epoch)\n",
    "\n",
    "def cosine_annealing(initial_lr, epoch, total_epochs):\n",
    "    \"\"\"\n",
    "    Cosine annealing (popular in transformers).\n",
    "    Smoothly decreases from initial_lr to 0.\n",
    "    \"\"\"\n",
    "    return initial_lr * 0.5 * (1 + np.cos(np.pi * epoch / total_epochs))\n",
    "\n",
    "def warmup_cosine(initial_lr, epoch, total_epochs, warmup_epochs=10):\n",
    "    \"\"\"\n",
    "    Linear warmup + cosine decay (used in GPT, BERT).\n",
    "    \n",
    "    Why warmup? Large learning rates at start can destabilize training.\n",
    "    Warmup gradually increases LR from 0 to initial_lr.\n",
    "    \"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warmup\n",
    "        return initial_lr * (epoch / warmup_epochs)\n",
    "    else:\n",
    "        # Cosine decay\n",
    "        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "        return initial_lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "# Visualize schedules\n",
    "total_epochs = 100\n",
    "initial_lr = 0.1\n",
    "epochs = np.arange(total_epochs)\n",
    "\n",
    "schedules = [\n",
    "    ('Constant', [constant_lr(initial_lr, e, total_epochs) for e in epochs]),\n",
    "    ('Step Decay', [step_decay(initial_lr, e, total_epochs) for e in epochs]),\n",
    "    ('Exponential', [exponential_decay(initial_lr, e, total_epochs) for e in epochs]),\n",
    "    ('Cosine', [cosine_annealing(initial_lr, e, total_epochs) for e in epochs]),\n",
    "    ('Warmup+Cosine', [warmup_cosine(initial_lr, e, total_epochs) for e in epochs]),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, schedule in schedules:\n",
    "    plt.plot(epochs, schedule, label=name, linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedules')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Warmup + Cosine is standard for transformers (GPT, BERT, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Which Optimizer to Use?\n",
    "\n",
    "| Optimizer | When to Use | Typical LR |\n",
    "|-----------|-------------|------------|\n",
    "| SGD | Simple problems, convex optimization | 0.01-0.1 |\n",
    "| SGD + Momentum | Computer vision (CNNs), when you need stability | 0.01-0.1 |\n",
    "| RMSprop | RNNs, non-stationary problems | 0.001 |\n",
    "| Adam | **Default choice**, transformers, most DL | 0.0001-0.001 |\n",
    "| AdamW | Transformers (better weight decay) | 0.0001-0.001 |\n",
    "\n",
    "**Rule of thumb**: Start with Adam (lr=0.001). If training is unstable, add warmup.\n",
    "\n",
    "**Your notes here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Gradient descent**: Follow negative gradient to minimize loss\n",
    "2. **Batch size matters**: Full batch (stable), SGD (noisy), mini-batch (practical)\n",
    "3. **Momentum**: Accelerates convergence, smooths path\n",
    "4. **Adaptive LR** (RMSprop/Adam): Different learning rates per parameter\n",
    "5. **Adam**: Best all-around optimizer (momentum + adaptive LR)\n",
    "6. **LR schedules**: Warmup prevents instability, cosine decay improves final performance\n",
    "\n",
    "**Your notes here:**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
